#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Complete CRUD Test for EpisodicMemoryRawRepository with KV-Storage

This test file comprehensively tests all CRUD methods in EpisodicMemoryRawRepository
with the dual MongoDB + KV-Storage pattern. Each test follows the pattern:
1. Create test data (append)
2. Read/Query test data
3. Verify data consistency between MongoDB and KV-Storage
4. Verify data integrity (inserted == retrieved)

Modified methods tested (8 total):
- get_by_event_id
- get_by_event_ids
- find_by_filters
- append_episodic_memory
- delete_by_event_id
- delete_by_user_id
- find_by_filter_paginated
"""

import asyncio
import pytest
import pytest_asyncio
import uuid
from datetime import timedelta
from typing import List, TYPE_CHECKING

# Mark all test functions in this module as asyncio tests
pytestmark = pytest.mark.asyncio

# Delay imports to avoid loading beanie at module level
# These will be imported inside fixtures/functions when needed
if TYPE_CHECKING:
    from infra_layer.adapters.out.persistence.document.memory.episodic_memory import (
        EpisodicMemory,
    )
    from infra_layer.adapters.out.persistence.repository.episodic_memory_raw_repository import (
        EpisodicMemoryRawRepository,
    )
    from infra_layer.adapters.out.persistence.kv_storage import KVStorageInterface


# ==================== Test Fixtures ====================


@pytest_asyncio.fixture
async def repository():
    """Get EpisodicMemoryRawRepository instance"""
    from core.di import get_bean_by_type
    from infra_layer.adapters.out.persistence.repository.episodic_memory_raw_repository import (
        EpisodicMemoryRawRepository,
    )
    repo = get_bean_by_type(EpisodicMemoryRawRepository)
    yield repo
    # Cleanup is handled by individual tests


@pytest_asyncio.fixture
async def kv_storage():
    """Get KV-Storage instance"""
    from core.di import get_bean_by_type
    from infra_layer.adapters.out.persistence.kv_storage import KVStorageInterface
    kv = get_bean_by_type(KVStorageInterface)
    yield kv


@pytest.fixture
def test_user_id():
    """Generate unique test user ID"""
    return f"test_user_{uuid.uuid4().hex[:8]}"


@pytest.fixture
def test_group_id():
    """Generate unique test group ID"""
    return f"test_group_{uuid.uuid4().hex[:8]}"


# ==================== Test Helpers ====================


def create_test_episodic_memory(
    user_id: str,
    summary: str = "Test episodic memory",
    episode: str = "Test episode content",
    group_id: str = None,
    participants: List[str] = None,
    keywords: List[str] = None,
    timestamp_offset: timedelta = None,
):
    """Helper function to create a test EpisodicMemory with all fields"""
    from common_utils.datetime_utils import get_now_with_timezone
    from infra_layer.adapters.out.persistence.document.memory.episodic_memory import (
        EpisodicMemory,
    )

    now = get_now_with_timezone()
    if timestamp_offset:
        now = now + timestamp_offset

    # Create complete EpisodicMemory structure
    return EpisodicMemory(
        # Core required fields
        user_id=user_id,
        timestamp=now,
        summary=summary,
        episode=episode,
        # Optional fields - user/group info
        user_name=f"TestUser_{user_id[-8:]}",
        group_id=group_id or f"group_{user_id}",
        group_name=f"TestGroup_{user_id[-8:]}",
        # Optional fields - event info
        participants=participants or [user_id, "Participant1", "Participant2"],
        type="Conversation",
        subject=f"Subject: {summary}",
        # Optional fields - metadata
        keywords=keywords or ["test", "episodic", "memory"],
        linked_entities=[
            f"entity_{uuid.uuid4().hex[:8]}",
            f"project_{uuid.uuid4().hex[:8]}",
        ],
        memcell_event_id_list=[
            f"memcell_{uuid.uuid4().hex[:8]}",
            f"memcell_{uuid.uuid4().hex[:8]}",
        ],
        # Optional fields - extension
        extend={
            "test_flag": True,
            "test_id": uuid.uuid4().hex,
            "priority": "high",
            "location": "Test Room",
        },
        # Vector fields (will be auto-generated if episode is provided)
        vector=None,  # Will be generated by vectorize_service
        vector_model=None,  # Will be set by vectorize_service
    )


def assert_episodic_memory_equal(em1, em2, check_id: bool = True):
    """Assert two EpisodicMemory objects are equal (comparing all fields)"""
    if check_id:
        assert str(em1.id) == str(em2.id), "IDs don't match"

    # Core required fields
    assert em1.user_id == em2.user_id, "user_id doesn't match"
    assert em1.summary == em2.summary, "summary doesn't match"
    assert em1.episode == em2.episode, "episode doesn't match"

    # Timestamps might have microsecond differences, allow small tolerance
    if em1.timestamp and em2.timestamp:
        time_diff = abs((em1.timestamp - em2.timestamp).total_seconds())
        assert time_diff < 1, f"timestamp difference too large: {time_diff}s"

    # Optional fields - user/group info
    assert em1.user_name == em2.user_name, "user_name doesn't match"
    assert em1.group_id == em2.group_id, "group_id doesn't match"
    assert em1.group_name == em2.group_name, "group_name doesn't match"

    # Optional fields - event info
    assert set(em1.participants or []) == set(
        em2.participants or []
    ), "participants don't match"
    assert em1.type == em2.type, "type doesn't match"
    assert em1.subject == em2.subject, "subject doesn't match"

    # Optional fields - metadata
    assert set(em1.keywords or []) == set(
        em2.keywords or []
    ), "keywords don't match"
    assert set(em1.linked_entities or []) == set(
        em2.linked_entities or []
    ), "linked_entities don't match"
    assert set(em1.memcell_event_id_list or []) == set(
        em2.memcell_event_id_list or []
    ), "memcell_event_id_list doesn't match"

    # Optional fields - extension
    assert em1.extend == em2.extend, "extend doesn't match"

    # Vector fields (vector might be auto-generated, just check existence)
    if em1.vector or em2.vector:
        assert (em1.vector is not None) == (
            em2.vector is not None
        ), "vector existence doesn't match"
        if em1.vector and em2.vector:
            assert len(em1.vector) == len(em2.vector), "vector length doesn't match"
    assert em1.vector_model == em2.vector_model, "vector_model doesn't match"


async def verify_kv_storage(repository, event_id: str) -> bool:
    """Verify data exists in KV-Storage"""
    from core.observation.logger import get_logger

    logger = get_logger(__name__)

    kv_storage = repository._dual_storage.get_kv_storage()
    if not kv_storage:
        logger.warning("KV-Storage not available")
        return False

    kv_json = await kv_storage.get(key=event_id)
    return kv_json is not None


# ==================== Test Cases ====================


def get_logger():
    """Helper to get logger instance"""
    from core.observation.logger import get_logger as _get_logger

    return _get_logger(__name__)


class TestBasicCRUD:
    """Test basic CRUD operations: Create, Read, Delete"""

    async def test_01_append_and_get_by_event_id(self, repository, test_user_id):
        """
        Test: append_episodic_memory + get_by_event_id
        Flow: Create an EpisodicMemory -> Read it back -> Verify data matches
        """
        logger = get_logger()
        logger.info("=" * 60)
        logger.info("TEST: append_episodic_memory + get_by_event_id")

        # 1. Create test EpisodicMemory
        original = create_test_episodic_memory(
            user_id=test_user_id,
            summary="Test memory for get_by_event_id",
            episode="This is a test episode for single retrieval",
        )

        # 2. Append to repository
        created = await repository.append_episodic_memory(original)
        assert created is not None, "Failed to append EpisodicMemory"
        assert created.id is not None, "Created EpisodicMemory should have ID"

        event_id = str(created.id)
        logger.info(f"✅ Created EpisodicMemory with ID: {event_id}")

        # 3. Verify KV-Storage
        kv_exists = await verify_kv_storage(repository, event_id)
        logger.info(f"KV-Storage: {'✅ Exists' if kv_exists else '⚠️  Not found'}")

        # 4. Read back using get_by_event_id
        retrieved = await repository.get_by_event_id(event_id, test_user_id)
        assert retrieved is not None, "Failed to retrieve EpisodicMemory"
        logger.info(f"✅ Retrieved EpisodicMemory by event_id")

        # 5. Verify data matches
        assert_episodic_memory_equal(created, retrieved, check_id=True)
        logger.info(f"✅ Data integrity verified")

        # Cleanup
        await repository.delete_by_event_id(event_id, test_user_id)

    async def test_02_append_and_get_by_event_ids(self, repository, test_user_id):
        """
        Test: append_episodic_memory + get_by_event_ids (batch read)
        Flow: Create 3 EpisodicMemories -> Batch read -> Verify all data matches
        """
        logger = get_logger()
        logger.info("=" * 60)
        logger.info("TEST: append_episodic_memory + get_by_event_ids")

        # 1. Create 3 test EpisodicMemories
        created_list = []
        for i in range(3):
            original = create_test_episodic_memory(
                user_id=test_user_id,
                summary=f"Test memory {i+1} for batch get",
                episode=f"This is test episode {i+1} for batch retrieval",
            )
            created = await repository.append_episodic_memory(original)
            assert created is not None
            created_list.append(created)
            logger.info(f"✅ Created EpisodicMemory {i+1}: {created.id}")

        event_ids = [str(em.id) for em in created_list]

        # 2. Batch read using get_by_event_ids
        result_dict = await repository.get_by_event_ids(event_ids, test_user_id)
        assert len(result_dict) == 3, f"Expected 3 results, got {len(result_dict)}"
        logger.info(f"✅ Batch retrieved {len(result_dict)} EpisodicMemories")

        # 3. Verify all data matches
        for created in created_list:
            event_id = str(created.id)
            retrieved = result_dict.get(event_id)
            assert (
                retrieved is not None
            ), f"EpisodicMemory {event_id} not found in results"
            assert_episodic_memory_equal(created, retrieved, check_id=True)

        logger.info(f"✅ All {len(created_list)} EpisodicMemories verified")

        # Cleanup
        for event_id in event_ids:
            await repository.delete_by_event_id(event_id, test_user_id)

    async def test_03_delete_by_event_id(self, repository, test_user_id):
        """
        Test: append_episodic_memory + delete_by_event_id + get_by_event_id
        Flow: Create -> Delete -> Verify deletion (MongoDB + KV)
        """
        logger = get_logger()
        logger.info("=" * 60)
        logger.info("TEST: delete_by_event_id")

        # 1. Create test EpisodicMemory
        original = create_test_episodic_memory(
            user_id=test_user_id,
            summary="Test memory to be deleted",
            episode="This episode will be deleted",
        )
        created = await repository.append_episodic_memory(original)
        assert created is not None
        event_id = str(created.id)
        logger.info(f"✅ Created EpisodicMemory: {event_id}")

        # 2. Verify it exists
        retrieved = await repository.get_by_event_id(event_id, test_user_id)
        assert retrieved is not None, "EpisodicMemory should exist before deletion"

        # 3. Delete the EpisodicMemory
        deleted = await repository.delete_by_event_id(event_id, test_user_id)
        assert deleted is True, "Deletion should return True"
        logger.info(f"✅ Deleted EpisodicMemory: {event_id}")

        # 4. Verify it no longer exists
        retrieved_after = await repository.get_by_event_id(event_id, test_user_id)
        assert (
            retrieved_after is None
        ), "EpisodicMemory should not exist after deletion"
        logger.info(f"✅ Verified deletion: EpisodicMemory not found")

        # 5. Verify KV-Storage cleanup
        kv_exists = await verify_kv_storage(repository, event_id)
        assert not kv_exists, "KV-Storage should be cleaned up"
        logger.info(f"✅ KV-Storage cleaned up")

    async def test_04_find_by_filters(self, repository, test_user_id):
        """
        Test: append_episodic_memory + find_by_filters
        Flow: Create 3 EpisodicMemories for user -> Query by user_id -> Verify results
        """
        logger = get_logger()
        logger.info("=" * 60)
        logger.info("TEST: find_by_filters")

        # 1. Create 3 EpisodicMemories for the same user
        created_list = []
        for i in range(3):
            original = create_test_episodic_memory(
                user_id=test_user_id,
                summary=f"Memory {i+1} for user query",
                episode=f"Episode {i+1} for user query",
                timestamp_offset=timedelta(minutes=i),
            )
            created = await repository.append_episodic_memory(original)
            created_list.append(created)

        logger.info(
            f"✅ Created {len(created_list)} EpisodicMemories for user: {test_user_id}"
        )

        # 2. Query by user_id
        results = await repository.find_by_filters(user_id=test_user_id)
        assert len(results) >= 3, f"Expected at least 3 results, got {len(results)}"
        logger.info(f"✅ Found {len(results)} EpisodicMemories for user")

        # 3. Verify all created EpisodicMemories are in results
        result_ids = {str(em.id) for em in results}
        for created in created_list:
            assert (
                str(created.id) in result_ids
            ), f"Created EpisodicMemory {created.id} not in results"

        logger.info(f"✅ All created EpisodicMemories found in query results")

        # Cleanup
        for created in created_list:
            await repository.delete_by_event_id(str(created.id), test_user_id)


class TestQueryMethods:
    """Test query methods: find by filters and time ranges"""

    async def test_05_find_by_time_range(self, repository, test_user_id):
        """
        Test: append_episodic_memory + find_by_filters (with time range)
        Flow: Create EpisodicMemories at different times -> Query by time range -> Verify
        """
        logger = get_logger()
        logger.info("=" * 60)
        logger.info("TEST: find_by_filters (time range)")

        from common_utils.datetime_utils import get_now_with_timezone

        now = get_now_with_timezone()

        # 1. Create EpisodicMemories at different times
        # Before range
        em_before = create_test_episodic_memory(
            user_id=test_user_id,
            summary="Before time range",
            episode="Episode before range",
            timestamp_offset=timedelta(hours=-2),
        )
        created_before = await repository.append_episodic_memory(em_before)

        # Inside range
        em_inside1 = create_test_episodic_memory(
            user_id=test_user_id,
            summary="Inside time range 1",
            episode="Episode inside range 1",
            timestamp_offset=timedelta(minutes=0),
        )
        created_inside1 = await repository.append_episodic_memory(em_inside1)

        em_inside2 = create_test_episodic_memory(
            user_id=test_user_id,
            summary="Inside time range 2",
            episode="Episode inside range 2",
            timestamp_offset=timedelta(minutes=5),
        )
        created_inside2 = await repository.append_episodic_memory(em_inside2)

        # After range
        em_after = create_test_episodic_memory(
            user_id=test_user_id,
            summary="After time range",
            episode="Episode after range",
            timestamp_offset=timedelta(hours=2),
        )
        created_after = await repository.append_episodic_memory(em_after)

        logger.info(f"✅ Created 4 EpisodicMemories at different times")

        # 2. Query with time range
        start_time = now - timedelta(minutes=10)
        end_time = now + timedelta(minutes=20)

        results = await repository.find_by_filters(
            start_time=start_time,
            end_time=end_time,
        )

        # 3. Verify only inside-range EpisodicMemories are returned
        result_ids = {str(em.id) for em in results}

        assert (
            str(created_inside1.id) in result_ids
        ), "Inside range 1 should be included"
        assert (
            str(created_inside2.id) in result_ids
        ), "Inside range 2 should be included"
        assert (
            str(created_before.id) not in result_ids
        ), "Before range should be excluded"
        assert (
            str(created_after.id) not in result_ids
        ), "After range should be excluded"

        logger.info(f"✅ Time range query verified: 2 inside, 2 outside")

        # Cleanup
        for created in [
            created_before,
            created_inside1,
            created_inside2,
            created_after,
        ]:
            await repository.delete_by_event_id(str(created.id), test_user_id)

    async def test_06_find_by_filter_paginated(self, repository, test_user_id):
        """
        Test: append_episodic_memory + find_by_filter_paginated
        Flow: Create 5 EpisodicMemories -> Paginated query -> Verify pagination
        """
        logger = get_logger()
        logger.info("=" * 60)
        logger.info("TEST: find_by_filter_paginated")

        # 1. Create 5 EpisodicMemories
        created_list = []
        for i in range(5):
            original = create_test_episodic_memory(
                user_id=test_user_id,
                summary=f"Memory {i+1} for pagination",
                episode=f"Episode {i+1} for pagination",
                timestamp_offset=timedelta(minutes=i),
            )
            created = await repository.append_episodic_memory(original)
            created_list.append(created)

        logger.info(f"✅ Created {len(created_list)} EpisodicMemories")

        # 2. Query first page (limit=2, skip=0)
        page1 = await repository.find_by_filter_paginated(
            query_filter={"user_id": test_user_id},
            skip=0,
            limit=2,
            sort_field="timestamp",
            sort_desc=False,
        )
        assert len(page1) == 2, f"Page 1 should have 2 results, got {len(page1)}"
        logger.info(f"✅ Page 1: {len(page1)} results")

        # 3. Query second page (limit=2, skip=2)
        page2 = await repository.find_by_filter_paginated(
            query_filter={"user_id": test_user_id},
            skip=2,
            limit=2,
            sort_field="timestamp",
            sort_desc=False,
        )
        assert len(page2) == 2, f"Page 2 should have 2 results, got {len(page2)}"
        logger.info(f"✅ Page 2: {len(page2)} results")

        # 4. Query third page (limit=2, skip=4)
        page3 = await repository.find_by_filter_paginated(
            query_filter={"user_id": test_user_id},
            skip=4,
            limit=2,
            sort_field="timestamp",
            sort_desc=False,
        )
        assert len(page3) >= 1, f"Page 3 should have at least 1 result, got {len(page3)}"
        logger.info(f"✅ Page 3: {len(page3)} results")

        # 5. Verify no duplicates across pages
        all_ids = set()
        for page in [page1, page2, page3]:
            for em in page:
                assert str(em.id) not in all_ids, "Found duplicate ID across pages"
                all_ids.add(str(em.id))

        logger.info(f"✅ Pagination verified: no duplicates across pages")

        # Cleanup
        for created in created_list:
            await repository.delete_by_event_id(str(created.id), test_user_id)


class TestBatchOperations:
    """Test batch operations: delete multiple records"""

    async def test_07_delete_by_user_id(self, repository):
        """
        Test: append_episodic_memory + delete_by_user_id + find_by_filters
        Flow: Create 3 EpisodicMemories for user -> Delete all by user -> Verify deletion
        """
        logger = get_logger()
        logger.info("=" * 60)
        logger.info("TEST: delete_by_user_id")

        # Use unique user ID for this test
        test_user = f"test_user_delete_{uuid.uuid4().hex[:8]}"

        # 1. Create 3 EpisodicMemories for the user
        created_list = []
        for i in range(3):
            em = create_test_episodic_memory(
                user_id=test_user,
                summary=f"Memory {i+1} to be deleted",
                episode=f"Episode {i+1} to be deleted",
            )
            created = await repository.append_episodic_memory(em)
            created_list.append(created)

        logger.info(f"✅ Created 3 EpisodicMemories for user: {test_user}")

        # 2. Verify count before deletion
        results_before = await repository.find_by_filters(user_id=test_user)
        count_before = len(results_before)
        assert count_before >= 3, f"Expected at least 3 records, got {count_before}"

        # 3. Delete all by user_id
        deleted_count = await repository.delete_by_user_id(test_user)
        assert (
            deleted_count >= 3
        ), f"Expected to delete at least 3, deleted {deleted_count}"
        logger.info(f"✅ Deleted {deleted_count} EpisodicMemories for user")

        # 4. Verify count after deletion
        results_after = await repository.find_by_filters(user_id=test_user)
        count_after = len(results_after)
        assert (
            count_after == 0
        ), f"Expected 0 records after deletion, got {count_after}"
        logger.info(f"✅ Verified deletion: count = 0")

        # 5. Verify individual EpisodicMemories are gone
        for created in created_list:
            retrieved = await repository.get_by_event_id(str(created.id), test_user)
            assert retrieved is None, f"EpisodicMemory {created.id} should be deleted"


class TestEdgeCases:
    """Test edge cases and error handling"""

    async def test_08_get_nonexistent_event_id(self, repository, test_user_id):
        """
        Test: get_by_event_id with non-existent ID
        Expected: Should return None
        """
        logger = get_logger()
        logger.info("=" * 60)
        logger.info("TEST: get_by_event_id (non-existent)")

        fake_id = "000000000000000000000000"
        result = await repository.get_by_event_id(fake_id, test_user_id)

        assert result is None, "Non-existent ID should return None"
        logger.info(f"✅ Non-existent ID handled correctly: returned None")

    async def test_09_delete_nonexistent_event_id(self, repository, test_user_id):
        """
        Test: delete_by_event_id with non-existent ID
        Expected: Should complete without error (return value depends on implementation)
        """
        logger = get_logger()
        logger.info("=" * 60)
        logger.info("TEST: delete_by_event_id (non-existent)")

        fake_id = "000000000000000000000000"
        result = await repository.delete_by_event_id(fake_id, test_user_id)

        # Note: Return value depends on implementation
        # MongoDB won't find it, but KV-Storage might return False for delete operation
        assert isinstance(result, bool), "Should return a boolean"
        logger.info(f"✅ Non-existent ID deletion handled correctly: returned {result}")

    async def test_10_verify_audit_fields(self, repository, test_user_id):
        """
        Test: Verify created_at and updated_at are set correctly
        Bug fix: After removing AuditBase from EpisodicMemoryLite, these fields should still be set
        """
        logger = get_logger()
        logger.info("=" * 60)
        logger.info("TEST: Verify created_at and updated_at fields")

        # 1. Create and append EpisodicMemory
        original = create_test_episodic_memory(
            user_id=test_user_id,
            summary="Test audit fields",
            episode="Test episode for audit fields",
        )
        created = await repository.append_episodic_memory(original)
        assert created is not None, "append_episodic_memory should return EpisodicMemory"

        # 2. Verify audit fields are set after append
        assert (
            created.created_at is not None
        ), "❌ BUG: created_at should not be None!"
        assert (
            created.updated_at is not None
        ), "❌ BUG: updated_at should not be None!"
        logger.info(
            f"✅ After append: created_at={created.created_at}, updated_at={created.updated_at}"
        )

        # 3. Retrieve from KV-Storage and verify persistence
        retrieved = await repository.get_by_event_id(str(created.id), test_user_id)
        assert retrieved is not None, "get_by_event_id should return EpisodicMemory"
        assert (
            retrieved.created_at is not None
        ), "❌ BUG: created_at should persist in KV-Storage!"
        assert (
            retrieved.updated_at is not None
        ), "❌ BUG: updated_at should persist in KV-Storage!"
        logger.info(
            f"✅ After retrieve: created_at={retrieved.created_at}, updated_at={retrieved.updated_at}"
        )

        # 4. Verify timezones are consistent
        assert (
            retrieved.created_at.tzinfo == retrieved.timestamp.tzinfo
        ), "created_at timezone should match timestamp"
        assert (
            retrieved.updated_at.tzinfo == retrieved.timestamp.tzinfo
        ), "updated_at timezone should match timestamp"
        logger.info(f"✅ Timezone consistency verified: {retrieved.timestamp.tzinfo}")

        # 5. Verify created_at equals updated_at for newly created records
        time_diff = abs((retrieved.created_at - retrieved.updated_at).total_seconds())
        assert (
            time_diff < 1
        ), "created_at and updated_at should be nearly identical for new records"
        logger.info(f"✅ created_at ≈ updated_at (diff: {time_diff:.6f}s)")

        # Cleanup
        await repository.delete_by_event_id(str(created.id), test_user_id)
        logger.info("✅ Audit fields verification passed")


# ==================== Main Test Runner ====================


if __name__ == "__main__":
    """
    Run all tests with pytest

    Usage:
        pytest tests/test_episodic_memory_crud_complete.py -v -s
    """
    pytest.main([__file__, "-v", "-s"])
